# -*- coding: utf-8 -*-
"""finalpro_(1) (3) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cWNxoZuK8HXw1hqPab2fRVNUdCJwhHjg
"""

!pip install yfinance

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import yfinance as yf
import datetime as dt
import pandas as pd
import numpy as np
from numpy import arange
import matplotlib.pyplot as plt
from pandas import read_csv
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV

sp500= yf.download("^GSPC", start="2016-01-01", end="2023-1-1")
sp500_df = pd.DataFrame(sp500)
sp500_df.to_csv("sp500_data.csv")

sp500.head()

data = pd.read_csv("sp500_data.csv")
bool_series = pd.notnull(data["Adj Close"])
data[bool_series]

#Rescaling Data
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
rescaledX=scaler.fit_transform(X)
np.set_printoptions(precision=3) 
rescaledX[0:5,:]

#standardization
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler().fit(X)
rescaledX=scaler.transform(X)
rescaledX[0:5,:]

#Normalizing Data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler().fit(X)
rescaledX=scaler.transform(X)
rescaledX[0:5,:]

#feature selection
X = sp500.iloc[ : , :-1].values
Y = sp500.iloc[ : , 3].values

from sklearn.preprocessing import scale
data_standardized=scale(sp500)
data_standardized.mean(axis=0)
data_standardized.std(axis=0)

sp500.head()

"""# Random Forest and LSTM"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from tensorflow.keras.utils import to_categorical

#Numpy is the package for scientific computing with python. Array computations are made very easy by using this package.
import numpy as np

#Functions from pandas_datareader.data and pandas_datareader.wb extract data from various Internet sources into a pandas DataFrame.
from pandas_datareader import data as wb

#Using pyplot, we can get interactive plots and generate programmatic plots
import matplotlib.pyplot as plt

PG = sp500
PG.head()

PG['Log Return'] = np.log(PG['Adj Close']/PG['Adj Close'].shift(1))*100

ln=PG['Log Return']

print(ln)

df2 = sp500_df.assign(ln_return=ln)
df2.head()

a=df2.drop(['Log Return'], axis=1)
data=a
data.head()
data.dropna()

data['returns'] = data['Adj Close'].pct_change()
re=data['returns']
df2 = sp500_df.assign(returns=re)
data=a
data.head()
data.dropna()

data['up_down'] = np.where(data['Volume'].shift(-1) > data['Volume'],1,0)
id=data['up_down']
df2 = sp500_df.assign(up_down=id)
data=a
data.head()
data.dropna()

#data=data.drop(['Increase_Decrease'], axis=1)
data.head()
data.dropna()

def ADX(data, period):  
    df = data[['Close','High','Low','TR']].copy()                               
    alpha = 1/period
    df['ATR'] = df['TR'].ewm(alpha=alpha, adjust=False).mean()

    # +-DX
    df['H-pH'] = df['High'] - df['High'].shift(1)
    df['pL-L'] = df['Low'].shift(1) - df['Low']
    df['+DX'] = np.where(
        (df['H-pH'] > df['pL-L']) & (df['H-pH']>0),
        df['H-pH'], 0.0    )
    df['-DX'] = np.where(
        (df['H-pH'] < df['pL-L']) & (df['pL-L']>0),
        df['pL-L'], 0.0    )
    del df['H-pH'], df['pL-L']

    # +- DMI
    df['S+DM'] = df['+DX'].ewm(alpha=alpha, adjust=False).mean()
    df['S-DM'] = df['-DX'].ewm(alpha=alpha, adjust=False).mean()
    df['+DMI'] = (df['S+DM']/df['ATR'])*100
    df['-DMI'] = (df['S-DM']/df['ATR'])*100
    del df['S+DM'], df['S-DM']

    # ADX
    df['DX'] = (np.abs(df['+DMI'] - df['-DMI'])/(df['+DMI'] + df['-DMI']))*100
    df['ADX'] = df['DX'].ewm(alpha=alpha, adjust=False).mean()
    del df['ATR'], df['-DX'], df['+DX'], df['+DMI'], df['-DMI']
    return df['DX'],df['ADX']

def rsi_n(data,n):
  delta=np.array(data.diff(1))
  delta[np.isnan(delta)]=0.000000001
  dUp, dDown = delta.copy(), delta.copy()
  dUp[dUp < 0] = 0
  dDown[dDown > 0] = 0
  RolUp = pd.DataFrame(dUp).rolling(n).mean()
  RolDown = pd.DataFrame(dDown).abs().rolling(n).mean()
  RS =RolUp / RolDown
  rsi= 100.0 - (100.0 / (1.0 + np.array(RS)))
  return rsi

def forceindex(data_c,data_v,ndays):
    ForceIndex=pd.Series(data_c.diff(1)* data_v)    
    return ForceIndex.ewm(ndays).mean()


def CCI(data_tp, ndays): 
    CCI = pd.Series((data_tp - data_tp.rolling(ndays).mean()) / (0.015 * data_tp.rolling(ndays).std()),name = 'CCI') 
    return CCI 

def money_flow_index(df, periods=14):
    data=df.copy()
    data['money_flow'] = np.array(data['TP']) * np.array(data['Volume'])
    # data['money_flow'] = data['TP'] * data['Volume']
    delta=data['money_flow'].diff(1)
    delta[np.isnan(delta)]=0.000000001
    dUp, dDown = delta.copy(), delta.copy()
    dUp[dUp < 0] = 0
    dDown[dDown > 0] = 0
    RolUp = pd.DataFrame(dUp).rolling(periods).mean()
    RolDown = pd.DataFrame(dDown).abs().rolling(periods).mean()
    MFR =RolUp / RolDown
    MFI= 100.0 - (100.0 / (1.0 + np.array(MFR)))
    return MFR, MFI

def negative_volume_index(data, periods=255, close_col='', vol_col=''):
    data['nvi'] = 0.
    
    for index,row in data.iterrows():
        if index > 0:
            prev_nvi = data.at[index-1, 'nvi']
            prev_close = data.at[index-1, close_col]
            if row[vol_col] < data.at[index-1, vol_col]:
                nvi = prev_nvi + (row[close_col] - prev_close / prev_close * prev_nvi)
            else: 
                nvi = prev_nvi
        else:
            nvi = 1000
        data.set_value(index, 'nvi', nvi)
    data['nvi_ema'] = data['nvi'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean()
    
    return data


def williams_r(data, periods=14, high_col='', low_col='', close_col=''):
    data['williams_r'] = 0.
    
    for index,row in data.iterrows():
        if index > periods:
            data.set_value(index, 'williams_r', ((max(data[high_col][index-periods:index]) - row[close_col]) / 
                                                 (max(data[high_col][index-periods:index]) - min(data[low_col][index-periods:index]))))
        
    return data


def on_balance_volume(data, trend_periods=21, close_col='', vol_col=''):
    for index, row in data.iterrows():
        if index > 0:
            last_obv = data.at[index - 1, 'obv']
            if row[close_col] > data.at[index - 1, close_col]:
                current_obv = last_obv + row[vol_col]
            elif row[close_col] < data.at[index - 1, close_col]:
                current_obv = last_obv - row[vol_col]
            else:
                current_obv = last_obv
        else:
            last_obv = 0
            current_obv = row[vol_col]

        data.set_value(index, 'obv', current_obv)

    data['obv_ema' + str(trend_periods)] = data['obv'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean()
    return data

def EMV(data, ndays): 
    dm = ((data['High'] + data['Low'])/2) - ((data['High'].shift(1) + data['Low'].shift(1))/2)
    br = (data['Volume'] / 100000000) / ((data['High'] - data['Low']))
    EMV = dm / br 
    EMV_MA = pd.Series(EMV.rolling(ndays).mean(), name = 'EMV') 
    data = data.join(EMV_MA) 
    return data

train_x=data.loc['2016-01-01':'2021-01-01']
test_x=data.loc['2021-01-01':'2022-01-01']

train_y=data.loc['2016-01-01':'2021-01-01'] 
test_y=data.loc['2021-01-01':'2022-01-01']

scaler=MinMaxScaler()  # 0~1 nomarlization
scaler.fit(np.array(train_x))

train_x_n=scaler.transform(train_x)
test_y_n=scaler.transform(test_x)


train_y_return=train_y['ln_return']
train_y_class=to_categorical(train_y['up_down'])

test_y_return=test_y['ln_return']
test_y_class=to_categorical(test_y['up_down'])


data_y=pd.concat([train_y,test_y])
data_x=pd.concat([train_x,test_x])

data_x_n=scaler.transform(data_x)

data_y_return=np.array(data_y['ln_return'])
data_y_class=to_categorical(data_y['up_down'])



train_section=len(test_x)
print(len(data_x_n),len(train_x),len(test_x))
print(test_y_return.shape,train_y.shape)

import os
import tensorflow as tf
import random, sys, random, pickle
from keras.models import Sequential, Model
from keras import optimizers
from keras.layers import Input, LSTM, Dense, Dropout
from keras.regularizers import l2, l1
from keras.layers import Reshape, Activation, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Lambda
from keras.layers import ELU, PReLU, LeakyReLU
from keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler, TensorBoard
from keras.optimizers import SGD, Adam, RMSprop
from keras.layers import concatenate
import keras.backend as K
from pandas_datareader import data as pdr
from tensorflow.python.client import device_lib
dataset=data


def create_timestep(dataset,n):
    dataX= []
    for i in range(len(dataset)-n+1):
        a = dataset[i:(i+n)]
        dataX.append(a)
    return np.array(dataX)
def count(data):  
    c0=0
    c1=0
    for i in range(len(data)):
        if np.argmax(data[i])==0:
            c0=c0+1
        elif np.argmax(data[i])==1:
            c1=c1+1
    return c0,c1
def label(data):
    k=[]
    for i in range(len(data)):
        if np.argmax(data[i])==0:
            k.append(0)
        elif np.argmax(data[i])==1:
            k.append(1)
    return np.array(k)


drop_rate=0.5
initial_rate=0.001
step_epoch=50
lrList = []
def step_decay(epoch):
    lrate = initial_rate
    if epoch >= step_epoch:
        lrate = initial_rate*drop_rate
    elif epoch >=step_epoch*2:
        lrate = lrate*drop_rate
    elif epoch >=step_epoch*3:
        lrate = lrate*drop_rate
    lrList.append(lrate)
    return lrate



def lstm_forest_model(var,index,data,seq):
  for i in range(var):
    if i==0:
      lf_datax=create_timestep(data[:,index[0]],seq)      
      lf_datax=lf_datax.reshape((-1,seq,1))      
    else:
      lf_datax_i=create_timestep(data[:,index[i]],seq).reshape((-1,seq,1))      
      lf_datax=np.concatenate((lf_datax,lf_datax_i),axis=2)           
  return lf_datax
        



def test_LFM(data_x,y_return,y_cross,train_section):
  
  var=1
  seq=50
  epoch=150

  lis=range(data_x.shape[1])  
  index=random.sample(lis,var)    
  print('index',index)
  lf_datax=lstm_forest_model(var,index,data_x,seq)  
  lr = LearningRateScheduler(step_decay)
  callbacks_list = [lr]
  section=train_section
  
  y_cross, y_return =y_cross[seq-1:], y_return[seq-1:]
  train_x2, test_x = lf_datax[:-section,:,:], lf_datax[-section:,:,:]
  train_y_cross2, test_y_cross=y_cross[:-section],y_cross[-section:]
  train_y_return2, test_y_return=y_return[:-section],y_return[-section:]  
  
  #train sample random choice
  index_random2=random.sample(range(train_x2.shape[0]),int(train_x2.shape[0]*7/8))  
  index_random=np.sort(index_random2)

  train_x=train_x2[index_random,:,:]    
  train_y_cross=train_y_cross2[index_random,:]    
  train_y_return=train_y_return2[index_random]
  # train_y_return=train_y_return2[index_random,:]
  
  # modeling
  K.clear_session() 
  main_input = Input(shape=(seq,var),name='main_input')
  m=LSTM(15,return_sequences=False)(main_input)
  
  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
  m=Dropout(0.3)(m)

  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(m)
  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)
  main_cross=Dropout(0.3)(m)
  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)
  main_cross=Dense(2,activation='softmax',name='main_cross')(main_cross)

  main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(m)
  main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(main_return)
  main_return=Dropout(0.3)(main_return)
  main_return=Dense(30,kernel_initializer='he_normal')(main_return)
  main_return=Dense(1,activation='linear',name='main_return')(main_return)
  # model.summary()
  model=Model(inputs=main_input,outputs=[main_cross,main_return])
  model.compile(optimizer='adam',loss={'main_cross':'categorical_crossentropy','main_return':'mean_squared_error'},
                  metrics=['accuracy'],loss_weights={'main_cross':8,'main_return':1})
  print("model : ",model.summary())
  history=model.fit({'main_input':train_x},{'main_cross':train_y_cross,'main_return':train_y_return},shuffle=False,callbacks=callbacks_list,
                    validation_split=1/8,epochs=epoch,batch_size=256,verbose=0)
  
  # model evaluation
  lfm_predict=model.predict(test_x)

  val_section=int(len(train_x)*1/8)
  val_result=model.evaluate(train_x[-val_section:],[train_y_cross[-val_section:],train_y_return[-val_section:]])
  print("test evaluate",model.evaluate(test_x,[test_y_cross,test_y_return]))
  print(len(lfm_predict[0]),len(lfm_predict[1]))
  model.save('save dir + stockanme+ num_var+ sequence+ -th lstm')   
  
  return lfm_predict,val_result




lfm_var13_se50=[]

import pickle
for p in range(103):
  print(p ,'-th complete')
  lfm_var13_se50.append(test_LFM(data_x_n,data_y_return,data_y_class,train_section))

from keras.utils import plot_model
from IPython.display import Image
var=1
seq=50
epoch=150

# Build the model
K.clear_session() 
main_input = Input(shape=(seq,var),name='main_input')
m=LSTM(15,return_sequences=False)(main_input)

m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)
m=Dropout(0.3)(m)

main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(m)
main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)
main_cross=Dropout(0.3)(m)
main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)
main_cross=Dense(2,activation='softmax',name='main_cross')(main_cross)

main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(m)
main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(main_return)
main_return=Dropout(0.3)(main_return)
main_return=Dense(30,kernel_initializer='he_normal')(main_return)
main_return=Dense(1,activation='linear',name='main_return')(main_return)

model=Model(inputs=main_input,outputs=[main_cross,main_return])

# Print model summary
plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')
Image(filename='model.png')

import numpy as np
import pandas as pd


# Calculate the log returns
data['Log Return'] = np.log(data['Adj Close']/data['Adj Close'].shift(1))*100

# Calculate the rolling average of the log returns
data['Rolling Mean'] = data['Log Return'].rolling(window=30).mean()

# Calculate the rolling standard deviation of the log returns
data['Rolling Std'] = data['Log Return'].rolling(window=30).std()

# Calculate the upper and lower Bollinger Bands
data['Upper Band'] = data['Rolling Mean'] + 2 * data['Rolling Std']
data['Lower Band'] = data['Rolling Mean'] - 2 * data['Rolling Std']

# Calculate the next day prediction
next_day_prediction = data['Adj Close'].iloc[-1] * np.exp(data['Rolling Mean'].iloc[-1]/100)

# Calculate the next week prediction
next_week_prediction = data['Adj Close'].iloc[-1] * np.exp(data['Rolling Mean'].iloc[-1]/100 * 5)

# Calculate the next month prediction
next_month_prediction = data['Adj Close'].iloc[-1] * np.exp(data['Rolling Mean'].iloc[-1]/100 * 22)

print(next_month_prediction)

import matplotlib.pyplot as plt

# Plot the adjusted close price, rolling mean, and Bollinger Bands
fig, ax = plt.subplots(figsize=(12,6))
ax.plot(data.index, data['Adj Close'], label='Adjusted Close')
ax.plot(data.index, data['Rolling Mean'], label='Rolling Mean')
ax.plot(data.index, data['Upper Band'], label='Upper Band')
ax.plot(data.index, data['Lower Band'], label='Lower Band')

# Plot the next day, week, and month predictions
ax.axhline(y=next_day_prediction, color='g', linestyle='--', label='Next Day Prediction')
ax.axhline(y=next_week_prediction, color='r', linestyle='--', label='Next Week Prediction')
ax.axhline(y=next_month_prediction, color='m', linestyle='--', label='Next Month Prediction')

# Set the title and legend
ax.set_title('Stock Price with Bollinger Bands')
ax.legend()

# Show the plot
plt.show()